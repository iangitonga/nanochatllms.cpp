 # nanochatllms.cpp

**nanochatllms.cpp** is a repository containing pure C++ implementations of Chat-LLMs
with less than 3 billion parameters. The goal is to provide implementation of quantised
small Chat-LLMs that can run efficiently on lower-end devices offline. The models are
implemented in fp16, 8-bit and 4-bit formats. This project was inspired by
[llama.cpp](https://github.com/ggerganov/llama.cpp) and [llama.c](https://github.com/karpathy/llama2.c)

## Implemented models
1. TinyLlama-1.1B-Chat-v0.4
2. Zephyr1.6B
